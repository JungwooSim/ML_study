---
title: "ISLR_chapter06_Linear Model Selection and Regularization"
author: "Big"
date: '2019.04.21'
output: html_document
---
##Lab 1: 부분집합(서브셋) 선택 방법(Subset Selection Methods)

###6.5.1 최상의 서브셋 선택(Best Subset Selection)
> * Best Subset 선택 기법을 Hitters 자료에 적용하여 작년도 성적과 관련된 다양한 통계를 기반으로 야구 선수의 Salary를 예측하고자 한다.

```{r}
library(ISLR)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```
* Salary 변수값에 NA(누락)이 있는지 확인 필요하다.
* NA가 59개가 확인 된다.
<br></br><br></br>

```{r}
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```
* na.omit() 함수를 사용해서 변수값에 NA가 있는 행을 제거한다.
<br></br><br></br>

```{r}
library(leaps)
regfit.full = regsubsets(Salary~., Hitters)
summary(regfit.full)
```
* leaps 라이브러리에 포함된 regsubsets() 함수는 주어진 수의 설명변수를 포함하는 **최고의 모델을 식별함으로써 best subset을 선택**한다.
* 여기서 best는 RSS를 사용하여 수량화 한다.
* 결과에서 별표는 주어진 변수가 해당 모델에 포함된다는 것을 나타낸다.
* 예를 들어, 위 결과를 보면 2-variable 모델은 Hits와 CRBI만 포함한다.
<br></br><br></br>

```{r}
regfit.full = regsubsets(Salary~., data=Hitters, nvmax=19)
reg.summary = summary(regfit.full)
```
* 기본적으로 regsubsets() 함수는 8-variable 까지만 모델에 대한 결과를 보여준다. nvmax 옵션을 사용하면 원하는 변수 수 만큼 결과를 얻을 수 있다.
<br></br><br></br>

```{r}
names(reg.summary)
```
* summary() 함수는 R^2, Cp, BIC도 제공한다.
* 전체적으로 가장 좋은 모델을 선택하기 위해 이 값들을 조사할 수 있다.
<br></br><br></br>

```{r}
reg.summary$rsq
```
* R^2 통계량은 모델에 포함된 변수가 단 한개인 경우 0.3214501에서 모든 변수가 포함된 경우 0.5461159 까지 증가한다.
* 예상대로 R^2 통계량은 포함되는 변수의 수가 늘어남에 따라 증가한다.
<br></br><br></br>

```{r}
par(mfrow=c(2,2))
plot(
  reg.summary$rss,
  xlab = "Number of Variables",
  ylab = "RSS",
  type = "l"
)
plot(
  reg.summary$adjr2,
  xlab = "Number of Variables",
  ylab = "Adjusted RSq",
  type = "l"
)
which.max(reg.summary$adjr2) # 벡터에서 값이 최대인 원소의 위치를 식별
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20) #이미 그러진 plot에 점 추가
plot(
  reg.summary$cp,
  xlab = "Number of Variables",
  ylab = "Cp",
  type = "l"
)
points(which.min(reg.summary$cp), reg.summary$adjr2[which.min(reg.summary$cp)], col="red", cex=2, pch=20) #이미 그러진 plot에 점 추가
plot(
  reg.summary$bic,
  xlab = "Number of Variables",
  ylab = "Bic",
  type = "l"
)
points(which.min(reg.summary$bic), reg.summary$adjr2[which.min(reg.summary$bic)], col="red", cex=2, pch=20) #이미 그러진 plot에 점 추가
```
* 모든 모델에 대한 RSS, Adjusted R^2, Cp, BIC를 한꺼번에 그려보면 어느 모델을 선택할지 결졍하는데 도움이 될 것이다.
* 그래프의 점들을 선으로 연결하려면 type="l" 옵션을 사용하면 된다.
* which.max() 함수는 벡터에서 값이 **최대인** 원소의 위치를 식별할때 사용한다.
* which.min() 함수는 벡터에서 값이 **최하인** 원소의 위치를 식별할때 사용한다.
<br></br><br></br>

```{r}
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
```
* regsubsets() 함수는 내장된 plot()함수를 가지며, 주어진 수의 설명변수를 갖는 최상의 모델에 포함되는 변수들을 나타내는데 사용될 수 있다.
* 이 때 최상의 모델은 BIC, Cp, Adjusted R^2, AIC에 따른 순위에 의해 정해진다.
* 각 그래프의 맨 위쪽 행은 최적의 모델에 따라 선택된 각 변수에 대한 검은색 사각형을 포함한다.
* 예를 들어, 몇 개의 모델이 -150에 가까운 BIC를 갖는다. 하지만 가장 낮은 BIC를 가지는 모델은 AtBat, Hits, Walks, CRBI, DivisionW, PutOuts만 포함하는 6-variable 모델이다.
<br></br><br></br>

```{r}
coef(regfit.full,6)
```
* coef() 함수를 사용하여 이 모델과 관련된 계수 추정치들을 볼 수 있다.
<br></br><br></br>

###6.5.2 전진 및 후진 단계적 선택(Forward and Backward Stepwise Selection)

```{r}
regfit.fwd = regsubsets(Salary~., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary~., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
```
* regsubsets() 함수는 method="forward" 또는 method="backward" 인자를 사용하여 전진, 후진 단계적 선택을 수행할때 사용할 수 있다.
* 예를 들어, forward 선택을 사용하여 최고의 1-variable 모델은 CBBI 변수만 포함하고 있고 최고의 2-variable은 CBBI, Hits를 포함한다.
<br></br><br></br>

```{r}
coef(regfit.full,6)
coef(regfit.fwd,6)
coef(regfit.bwd,6)

coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
```
* 1-variable ~ 6-variable 까지는 best subsets, forward, backward 모두 같은 변수를 선택하지만 7-variable부터는 다른것을 알 수 있다.
<br></br><br></br>

###6.5.3 검증셋 기법과 교차검증을 사용한 모델 선택(Choosing Among Models Using the Validation Set Approach and Cross-Validation)
<br></br><br></br>
>* 조금전 크기가 다른 모델 중 Cp, BIC, Adjested R^2을 사용하여 선택하는 것이 가능하다는 것을 보았다.
>* 검증셋 기법과 교차검증을 사용한 모델 선택에 대해 고려해볼 것이다.
>* 이러한 기법들로 **정확한 검정오차 추정치를 얻기 위해서는 훈련 관측치만을 사용하여 변수 선택을 포함한 모델적합의 모든 것을 수행해야 한다.**
>* 그러므로 주어진 크기의 모델 중 어느 것이 최고인지는 훈련 관측치만을 사용하여 결정하여야 한다.
>* 이것은 미묘하지만 중요하다. 만약 Best Subsets 선택에 전체 자료가 사용된다면, 얻게 되는 검증셋 오차와 교차검증 오차는 검정오차의 추정치가 아닐 것이다.

```{r}
set.seed(1)
train = sample(c(TRUE,FALSE), nrow(Hitters), rep=TRUE)
test = (!train)
```
* 검증셋 기법을 사용하기 위해서는 관측치들을 훈련셋과 검정셋으로 분할한다.
<br></br><br></br>

```{r}
regfit.best = regsubsets(Salary~., data=Hitters[train,], nvmax=19)
```
* regsubsets()를 훈련셋에 적용
* Hitters[train, ]을 사용하여 Hitters 데이터 프레임에서 훈련 서브셋만 액세스한다.
<br></br><br></br>

```{r}
test.mat = model.matrix(Salary~., data=Hitters[test,])
```
* model.matrix() 함수는 많은 회귀 패키지에서 사용되며 데이터로부터 "X" 행렬을 구성한다.
* 이제 각 모델 크기에서 최고의 모델에 대한 검증셋 오차를 계산한다.
* 먼저 검증 데이터로부터 모델 행렬을 만든다.
<br></br><br></br>

```{r}
val.errors=rep(NA,19)
for(i in 1:19){
  coefi = coef(regfit.best, id=i)
  pred = test.mat[, names(coefi)]%*%coefi
  val.errors[i] = mean( (Hitters$Salary[test] - pred)^2 ) #MSE
}
val.errors
which.min(val.errors)
coef(regfit.best, which.min(val.errors))
```
* 루프를 실행하여 크기 i의 최고 모델에 대한 계수들을 regfit.best에서 추출하고,
* 이 계수들을 검정모델 행렬의 적절한 열에 곱하여 예측값을 구한다.
* 그다음에 검정 MSE를 계산한다.
<br></br><br></br>

```{r}
predict.regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  xvars = names(coefi)
  mat[, xvars]%*%coefi
}
```
* 위 과정은 약간 불편하다.
* regsubsets()에 대한 predict() 멤버함수(method)가 없기 때문이다.
* 나중에 다시 사용할 수 있도록 함수를 생성할 수 있다.
<br></br><br></br>

```{r}
regfit.best = regsubsets(Salary~., data=Hitters, nvmax = 19)
coef(regfit.best, 10)
```
* 마지막으로 전체 자료에서 best subset을 선택을 수행하고 최고의 10-variable 모델을 선택한다.
* 더 정확한 계수 추정치를 얻기 위해서는 전체 자료를 사용하는 것이 중요하다.
* 훈련셋이 아니라 전체 자료를 사용하는 이유는 전체 자료에 최고의 10-variable 모델은 훈련셋에서 얻은 모델과 다를 수 있기 때문이다.
* 전체 자료의 10-variable과 훈련셋의 10-variable은 다를 수도 있다.
<br></br><br></br>

```{r}
k=10
set.seed(1)
folds = sample(1:k, nrow(Hitters), replace=TRUE)
cv.errors = matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))
cv.errors
```
* 교차검증을 사용하여 크기가 다른 모델들 중에서 선택하는 것을 해보자.
* 이 방법은 k 훈련셋 각각에 대한 최상의 서브셋을 선택해야 하므로 다소 복잡하다.
* 그렇지만 R에서는 교묘한 서브셋 선택 문법 덕분에 상당히 쉽게 할 수 있다.
* 먼저, 각 관측치를 k = 10 fold 중의 하나에 할당하는 벡터를 생성하고 그 결과를 저장할 행렬을 생성한다.
<br></br><br></br>

```{r}
for(j in 1:k){
  best.fit = regsubsets(Salary~., data=Hitters[folds!=j, ], nvmax=19)
  for(i in 1:19){
    pred = predict(best.fit, Hitters[folds==j, ], id=i)
    cv.errors[j,i] = mean( (Hitters$Salary[folds==j] - pred)^2 )
  }
}
```
* 교차검증을 수행하는 for 루프를 작성한다.
* j번째 fold에서 j와 동일한 folds의 원소들은 검정셋에 있고 나머지는 훈련셋에 있다.
* 각 모델 크기에 대해 예측을 수행하고(위에서 작성한 predict()함수를 사용),
* 적절한 서브셋에 대한 검정오차를 계산하여 그것을 행렬 cv.errors 내에 저장한다.
<br></br><br></br>

```{r}
mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type="b")
```
* 이 결과 10 x 19 행렬이 얻어지며, 원소 (i,j)는 i번째 교차검증 fold와 최고의 j-variable 모델에 대한 검정 MSE이다.
* apply() 함수를 사용하여 이 행렬의 열 별로 평균을 구하면 벡터가 얻어지는데, 이 벡터의 j번째 원소는 j-variable 모델에 대한 교차검증 오차이다.
* 교차검증은 11-variable 를 선택한다.
<br></br><br></br>

```{r}
reg.best = regsubsets(Salary~., data=Hitters, nvmax=19)
coef(reg.best, 11)
```
* best subset 선택을 수행하여 얻은 11-variable 모델을 얻는다.
<br></br><br></br>

---

##Lab 2: Ridge Regression and the Lasso

>* Ridge Regression 과 lasso를 수행하기 위해 glmnet 패키지를 사용할 것이다.
>* glmnet 패키지의 주요 함수는 glmnet()으로, 능형회귀모델, lasso 모델 등의 적합에 사용될 수 있다.
>* 능혀회귀와 lasso를 수행하여 Hitters 데이터에 대해 Salary를 예측할 것이다.

```{r}
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))

x = model.matrix(Salary~., Hitters)[,-1]
y = Hitters$Salary
```
* 6.5절에서 기술된 것처럼 값이 누락된 것은 데이터에서 제외한다.
* model.matrix() 함수는 x를 생성하는데 유용하다. **이 함수는 19개의 설명변수에 대응하는 행렬을 제공할 뿐만 아니라 자동적으로 질적 변수를 가변수로 변환한다.**
* **glmnet()은 수치적 입력만 가질수 있으므로 질적 변수를 가변수로 변환해주는 것이 중요하다.**
<br></br><br></br>

###6.6.1 Ridge Regression

```{r}
library(glmnet)
grid = 10^seq(10,-2, length=100) # seq(10,-2, length=100) 는 10 ~ -2 가 되도록 100 등분으로 나누어서 반복
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid)
```
* glmnet() 함수는 모델의 유형을 결정하는 전달인자인 alpha를 가진다.
* alpha = 0 이면 능형회귀모델을 적합, alpha = 1 이면 lasso 모델을 적합한다.
* glmnet() 함수는 기본적으로 변수들을 표준화하여 scale이 동일하게 한다. 기본설정을 끄려면 standardize=FALSE를 사용하면 된다.
<br></br><br></br>

```{r}
dim(coef(ridge.mod))
```
* 20 x 100 행렬을 가지는 것을 볼 수 있다.
* 20행은 각 설명변수와 절편에 하나씩, 100열은 lambda 값에 대해 하나씩 가진다.
<br></br><br></br>

```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```
* 람다 값이 클 때 계수 추정치의 l2 norm은 작은 람다 값이 사용된 경우보다 훨씬 작을 것으로 예상된다.
* 람다 = 11497.57 일 때 계수들과 이들의 l2 norm을 보여준다.
<br></br><br></br>

```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```
* 람다 = 705.4802 일 때 계수들과 이들의 l2 norm을 보여준다.
* 위의 결과와 비교해보면 작은 람다 값에 관련된 계수들의 l2 norm이 훨씬 크다.(6.360612 < 57.11001)
<br></br><br></br>

```{r}
predict(ridge.mod, s=50, type="coefficients")[1:20,]
```
* 여러 가지 목적을 위해 predict() 함수를 사용할 수 있다.
* 예를 들어, 새로운 람다값, 이를테면 50에 대한 릿지회귀계수를 얻을 수 있다.
<br></br><br></br>

```{r}
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]
```
* 이제, 표본을 훈련셋과 검정셋으로 분할하여 릿지회귀와 lasso의 검정오차를 추정한다.
* 자료를 랜덤으로 분할하는데 많이 사용되는 두가지 방법이 있다.
1. TRUE, FALSE 원소로 구성되는 렌덤벡터를 생성하여 TRUE에 대응하는 관측치들을 훈련셋으로 사용한다.
2. 1과 n 사이 숫자들의 서브셋을 랜덤으로 선택하여 이들을 훈련 관측치들의 인덱스로 사용한다.
* 여기서는 2번째 방법을 사용한다.(6.5.3절에서 1번 방법을 사용했었음)
<br></br><br></br>

```{r}
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred = predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred - y.test)^2)
```
* 람다 = 4 를 사용하여 훈련셋에 릿지회귀모델을 적합하고 검정셋으로 MSE를 평가한다.
* predict() 함수를 사용하는데 type="coefficients" 를 newx 인자로 바꾸어 검정셋에 대한 추정치를 얻는다.
* 검정 MSE는 101036.8 이다.
<br></br><br></br>

```{r}
mean((mean(y[train])-y.test)^2)
```
* 만약 절편만 가진 모델을 단순히 적합했다면 훈련 관측치들의 평균을 사용하여 각 검정 관측치를 예측했을 것이다.(위 소스처럼..)
<br></br><br></br>

```{r}
ridge.pred = predict(ridge.mod, s=1e10, newx = x[test,])
mean((ridge.pred - y.test)^2)
```
* 람다 값이 매우 큰 능형회귀모델을 적합하여 동일한 결과를 얻을 수 있다. (1e10은 10^10을 의미)
<br></br><br></br>

>* 따라서 람다 = 4인 릿지회귀모델을 적합하는 것은 절편만 가진 모델을 적합하는 것보다 훨씬 낮은 검정 MSE가 나온다.
>* 최소제곱회귀 대신에 람다 = 4인 릿지회귀모델을 수행하는 것이 어떤 잇점이 있는지 체크해보자.(최소제곱은 람다 = 0 인 릿지회귀다.)

<br></br><br></br>

```{r}
ridge.pred = predict(ridge.mod, s = 0, newx=x[test,])
mean((ridge.pred - y.test)^2)
lm(y~x, subset = train)
predict(ridge.mod, s=0, type="coefficients")[1:20,]
```
* 일반적으로, 최소제곱모델을 적합하고자 한다면 lm() 함수를 사용해야 한다.
* 이유는 lm() 함수는 계수들에 대한 표준오차와 p-value 같은 더 유용한 결과를 제공해주기 때문이다.
<br></br><br></br>

```{r}
set.seed(1)
cv.out = cv.glmnet(x[train, ], y[train], alpha=0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y[test])^2)
```
* 조율 파라미터인 람다를 임의로 4로 하였었다. 임의로 선택하는 것보다 교차검증을 사용하여 선택하는 것이 보통 더 나을 것이다.
* cv.glmnet()을 사용하여 조율 파라미터를 선택할 수 있다.(이 함수는 기본적으로 10-fold 교차검증을 수행하는데, nfolds 인자를 사용하여 변경가능하다.)
* 검증오차가 가장 낮은 람다값은 211.7416 이다.
* 람다값 211.7416 설정한 검정 MSE는 96015.51 이다. 람다=4 보다 낮은 검정 MSE를 확인할 수 있다.
<br></br><br></br>

```{r}
out = glmnet(x, y, alpha=0)
predict(out, type="coefficients", s=bestlam)[1:20,]
```
* 교차검증에 의해 선택된 람다 값을 사용하여 전체 자료에 릿지회귀모델을 적합하고 계수 추정치를 조사한다.
* 릿지회귀모형은 수축할때 계수들을 0으로 만들지 않는다는 것을 확인 할 수 있다.
<br></br><br></br>

###6.6.2 Lasso
>* 릿지회귀는 람다를 잘 선택하면 최소제곱과 null model 보다 성능이 나을 수 있다는 것을 살펴보았다.
>* 이제 lasso가 릿지회귀보다 더 정확하거나 더 해석이 쉬운 모델을 제공할 수 있는지 알아본다.

```{r}
lasso.mod = glmnet(x[train,], y[train], alpha = 1)
plot(lasso.mod)
```
* lasso 모델 적합에도 glmnet() 함수를 사용한다.
* 계수 그래프로부터 알 수 있듯이, 조율 파라미터의 선택에 따라 몇몇 계수는 그 값이 정확하게 0이 될것이다.
<br></br><br></br>

```{r}
set.seed(1)
cv.out = cv.glmnet(x[train, ], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)
```
* 교차검증을 수행하여 최적의 람다값을 찾고 검정오차를 계산한다.
* 결과는 null model과 최소제곱의 검정셋 보다 MSE는 훨씬낮지만 릿지회귀모형보다는 약간 높다.
<br></br><br></br>

```{r}
out = glmnet(x, y, alpha=1, lambda = grid)
lasso.coef = predict(out, type="coefficients", s = bestlam)[1:20, ]
lasso.coef
lasso.coef[lasso.coef != 0]
```
* 릿지회귀모형보다는 MSE가 약간 높지만 0 이 아닌 계수 추정치의 수가 적기 때문에 상당한 이점이 있다.
<br></br><br></br>

##Lab 3: PCL and PLS Regression


Lab 3: PCR and PLS Regression
>* PCL은 pls 라이브러리의 pcr() 함수를 사용하여 수행할 수 있다.
>* PCL을 Hitters 자료에 적용하여 Salary를 예측한다.

###6.7.1 Principal Components Regression

```{r}
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```
* 누락된 값 제거
<br></br><br></br>

```{r}
library(pls)
set.seed(2)
pcr.fit = pcr(Salary~., data=Hitters, scale=TRUE, validation="CV")
summary(pcr.fit)
```
* pcr() 함수의 문법은 몇 가지 추가적인 욥션외에는 lm()과 유사하다.
* scale = TRUE 는 주성분을 생성하기 전에 각 설명변수를 (식 6.6)을 사용하여 표준화하므로 변수의 측정 스케일에 의한 영향은 없을 것이다.
* validation="CV"로 설정하면 pcr()은 사용된 주성분의 수 M에 대한 10-fold 교차검증 오차를 계산한다.
* 적합결과는 summary() 함수를 사용하여 조사할 수 있다.
* CV 값은 M = 0 에서부터 가능한 각 주성분의 수에 대해 제공된다.
* pcr()은 제곱근평균제곱오차(RMSE, root mean squared error)를 제공하므로 MSE를 얻을려면 이 값을 제곱해야 한다.
* 예를 들어, RMSE가 352.8 이라면 MSE는 352.8^2 = 124467.8 이다.
<br></br><br></br>

```{r}
validationplot(pcr.fit, val.type="MSEP")
```
* 교차검증 결과는 validationplot() 함수를 사용하여 그래프로 나타낼수도 있다. val.type="MSEP"를 사용하면 교차검증 MSE가 그래프로 표현될 것이다.
* M = 16일 때 교차검증 오차가 가장 적다. 이것은 M = 19일 때와 거의 차이가 없다. M = 19이면 단순히 최소제곱을 수행하는 것이 된다.
* 왜냐하면, PCR에서 모든 성분이 사용될 때는 차원축소가 없기 때문이다.
* 하지만, 그래프를 살펴보면 교차검증 오차는 하나의 성분만 포함하는 모델과 거의 같다는 것을 알 수 있다.
* 이것은 작은 수의 성분을 사용하는 모델이면 충분할 수 있다는 것을 시사한다.
<br></br><br></br>

```{r}
set.seed(1)
pcr.fit = pcr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
```
* 훈련셋에 대해 PCR을 수행하고 검정셋으로 성능을 평가한다.
<br></br><br></br>

```{r}
pcr.pred = predict(pcr.fit, x[test,], ncomp=7)
mean((pcr.pred-y.test)^2)
```
* 교차검증 오차가 가장 낮은 것은 M = 7개의 주성분이 사용된 경우다.
* 검정 MSE 계산한다.
* PCR 검정 MSE는 릿지회귀와 lasso와 결과가 유사하다.
* 하지만, PCR은 변수 선택을 수행하지 않고 심지어 직접적으로 계수 추정치도 제공하지 않기 때문에 모델 해석이 어렵다.
<br></br><br></br>

```{r}
pcr.fit = pcr(y~x, scale=TRUE, ncomp=7)
summary(pcr.fit)
```
* 마지막으로, 교차검증에 의해 선택된 주성분의 수 M = 7을 사용하여 PCR을 전체 자료에 적합한다.
<br></br><br></br>

###6.7.2 부분최소제곱(PLS, Partial Least Squares)
>* pls 라이브러리에 포함되어 있는 plsr() 함수를 사용하여 PLS을 수행한다.
>* 문법은 pcr() 함수와 마찬가지다.

```{r}
set.seed(1)
pls.fit = plsr(Salary~., data=Hitters, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP")
```
* 가장 낮은 교차검증 오차는 M=2개의 PCL 방향이 사용된 경우에 발생한다.
<br></br><br></br>

```{r}
pls.pred = predict(pls.fit, x[test,], ncomp=2)
mean((pls.pred-y.test)^2)
```
* 대응하는 검정셋 MSE를 평가한다.
* 검정 MSE는 릿지회귀, lasso, PCR을 사용하여 얻은 검정 MSE보다 약간 높기는 하지만 비슷한 수준이다.
<br></br><br></br>

```{r}
pls.fit = plsr(Salary~., data = Hitters, scale=TRUE, ncomp=2)
summary(pls.fit)
```
* 마지막으로, 교차검증에 의해 선택된 성분의 수 M = 2를 사용하여 전체 자료에 PLS를 수행한다.
* PLS 적합의 두 성분이 설명하는 Salary 내 분산의 백분율은 46.40%로, 7개 주성분을 사용한 PCR 적합의 46.69%와 거의 비슷하다.
* 이러한 결과는 PCR은 설명변수에서 설명되는 분산의 양만 최대로 하려고 하지만 PLS는 설명변수와 반응변수 둘 다의 분산을 설명하는 방향을 찾기 때문이다.



